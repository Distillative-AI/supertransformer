# Elastic Supertransformation Platform Demo: Calling nvapi from Elastic Provisioner Transformer

This notebook demonstrates how to use `nvapi` with an elastic provisioner to generate output from a model. We will provide examples using different models.

## Prerequisites

Ensure that you have `nvapi` and `eprov` installed and properly configured in your environment.

```python
import subprocess

# Define the function to install Elastic Provisioner
def install_eprov():
    command = 'curl https://api.elasticprovisioner.com/install-eprov | bash'

    # Execute the command
    result = subprocess.run(command, shell=True, capture_output=True, text=True)
    # Return the result
    return result.stdout

# Run the function and display the result
output = install_eprov()
print("Command Output:\n", output)
```

## Basic EPT Usage Example

We'll start with a basic example that echoes a rhyme about GPUs using the specified model. Here we're using Elastic Provisioner Transformer on the Elastic Supertransformation Platform to call these models via Nvidia API

```python
import subprocess

# Define the function to run the command
def run_nvapi_command():
    # Define the command to use the default model.
    command = 'echo "make a rhyme about GPUs" | eprov ept nvapi'
    
    # Execute the command
    result = subprocess.run(command, shell=True, capture_output=True, text=True)
    
    # Return the result
    return result.stdout

# Run the function and display the result
output = run_nvapi_command()
print("Command Output:\n", output)
```

## Example with Multiple Models

You can specify multiple models in a single `nvapi::` call by enclosing them in single quotes after the `::`. In this example, we use both the `mixtral` model and the default NVIDIA model.

```python
def run_nvapi_with_multiple_models():
    # call two models and get both results
    command = (
        'echo "make a rhyme about GPUs" | eprov ept nvapi::model=mistralai/mixtral-8x22b-instruct-v0.1 nvapi::model=meta/llama-3.1-8b-instruct'
    )
    
    # Execute the command
    result = subprocess.run(command, shell=True, capture_output=True, text=True)
    
    # Return the result
    return result.stdout

# Run the function and display the result
output = run_nvapi_with_multiple_models()
print("Command Output:\n", output)
```

## Conclusion

This notebook provides a basic guide to using `nvapi` with an elastic provisioner, along with examples of specifying different models. Customize the commands and parameters as needed for your specific use case.

This notebook focuses on:
- Specifying different models, including both `mixtral` and the default NVIDIA model, in a single call.
- A section to read and display the content of the uploaded file for additional information.
